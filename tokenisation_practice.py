# -*- coding: utf-8 -*-
"""Tokenisation_Practice.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11Ou7OWdDFsfHjOxo_WEW57oC20WwcO4l
"""

!pip install nltk

corpus=""" hello i am dixitha,please do .   support me in my career to step up everyday,
thank you god for giving me such a great life.


"""

print(corpus)

# convert sentense into para
import nltk
from nltk.tokenize import sent_tokenize
nltk.download('punkt')

documents=sent_tokenize(corpus)

type(documents)

# convert para into words
#sentense into words

from nltk.tokenize import word_tokenize

word_tokenize(corpus)

for sentense in documents:
  print(word_tokenize(sentense))

from nltk.tokenize import wordpunct_tokenize

wordpunct_tokenize(corpus)

from nltk.tokenize import TreebankWordTokenizer

tokenizer=TreebankWordTokenizer()

tokenizer.tokenize(corpus)

